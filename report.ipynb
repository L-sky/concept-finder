{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Place for problem statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Recap of readme.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Code\n",
    ">> Data preparation/processing \n",
    "\n",
    ">> Forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/usr/local/spark/spark-2.4.3-bin-without-hadoop'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "import pyspark\n",
    "findspark.find() # solely to check whether path is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SQLContext\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.ml.feature import BucketedRandomProjectionLSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fasttext\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirpath = os.getcwd()\n",
    "model_bin_path = f'{dirpath}/fasttext/crawl-300d-2M-subword.bin'\n",
    "emb_length = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.\\\n",
    "    master('yarn').\\\n",
    "    appName('scholar').\\\n",
    "    getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "# enables cartesian (!) join\n",
    "spark.conf.set(\"spark.sql.crossJoin.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pivot_keyterms(df):\n",
    "    # year - single integer, entities - list of strings (keyterms)\n",
    "    # keep only papers for periods (years) that fully passed\n",
    "    # make separate row for each keyterm\n",
    "    df = df.select('year', 'entities').\\\n",
    "        filter(\"year < 2019\").\\\n",
    "        withColumn('entities', F.explode('entities'))\n",
    "    \n",
    "    # turn all keyterms to lowercase\n",
    "    df = df.withColumn('entities', F.lower(F.col('entities')))\n",
    "    \n",
    "    # cut number formatting: 10,000 -> 10000\n",
    "    df = df.withColumn('entities', F.regexp_replace(F.col('entities'), '(\\d)[, ](\\d{3})', '$1$2'))\n",
    "    \n",
    "    # treat comma separated values as a different keyterms (even though they were posed as a unit)\n",
    "    df = df.withColumn('entities', F.explode(F.split('entities', '[,]')))\n",
    "    \n",
    "    # substitute underscore with a space as a means of separation\n",
    "    # get rid of excessive spaces\n",
    "    df = df.withColumn('entities', F.regexp_replace(F.col('entities'), '_', ' '))\n",
    "    df = df.withColumn('entities', F.trim(F.col('entities')))\n",
    "    df = df.withColumn('entities', F.regexp_replace(F.col('entities'), '\\s+', ' '))\n",
    "\n",
    "    # keep only keyterms that contain alpha-numeric values and spaces\n",
    "    # retrieve counts for each keyterm for each year\n",
    "    # \\w also includes uderscore, but we handled it earlier\n",
    "    df = df.filter(~(F.col('entities').rlike('[^\\w\\s]'))).\\\n",
    "        groupby('entities').\\\n",
    "        pivot('year').\\\n",
    "        count().\\\n",
    "        sort('entities')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens_frequency(df):\n",
    "    # keep only tokens starting from 3 characters in length\n",
    "    df = df.filter('LENGTH(entities) > 2')\n",
    "\n",
    "    # gather column names linked to years\n",
    "    col_years = [col_name for col_name in df.columns]\n",
    "    col_years.remove('entities')\n",
    "\n",
    "    # Find peak usage of token across the years\n",
    "    # https://stackoverflow.com/questions/40874657/pyspark-compute-row-maximum-of-the-subset-of-columns-and-add-to-an-exisiting-da\n",
    "    minf = F.lit(float(\"-inf\"))\n",
    "    df = df.withColumn(\"year_max\", F.greatest(*[F.coalesce(F.col(year), minf) for year in col_years]))\n",
    "\n",
    "    # forget about tokens that have never been really used\n",
    "    df = df.filter(\"year_max > 10\").drop('year_max')\n",
    "\n",
    "    # find total number of \"valid\" tokens used on each year\n",
    "    df = df.join(df.groupby().sum(*col_years))\n",
    "\n",
    "    # retrieve token frequency (times common coefficient) for each year\n",
    "    # coefficient is to make sure we do not limitations of float precision too hard\n",
    "    for year in col_years:\n",
    "        df = df.withColumn(year, 100000.0*F.col(year) / F.col(f'sum({year})')).drop(f'sum({year})')\n",
    "    \n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.\\\n",
    "    master('local[2]').\\\n",
    "    appName('scholar').\\\n",
    "    getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(pd_df):\n",
    "    # retrieve embeding for each keyterm\n",
    "    model = fasttext.load_model(model_bin_path)\n",
    "    pd_df['embeddings'] = pd_df['entities'].apply(model.__getitem__)\n",
    "    del model\n",
    "\n",
    "    # append embeddings as another column to the data frame \n",
    "    emb_components = pd.DataFrame(pd_df['embeddings'].tolist(), columns=[f'v{i}' for i in range(emb_length)])\n",
    "    pd_df['embeddings'] = pd_df['embeddings'].apply(lambda x: list(x))\n",
    "    pd_df = pd.concat([pd_df[:], emb_components[:]], axis=1) \n",
    "    return pd_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_normed_vector(x):\n",
    "    # make L2-norm of the vector to be unit\n",
    "    x_np = np.array(x, dtype=np.float64)\n",
    "    x_np = x_np / np.linalg.norm(x_np)\n",
    "    return Vectors.dense(x_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_lsh_model(df):\n",
    "    df = df.select('entities', 'embeddings')\n",
    "\n",
    "    to_vector = F.udf(lambda x: Vectors.dense(x), VectorUDT())\n",
    "    to_normed_vector = F.udf(make_normed_vector, VectorUDT())\n",
    "\n",
    "    df = df.withColumn('normed_embeddings', to_normed_vector('embeddings'))\n",
    "    df = df.withColumn('embeddings', to_vector('embeddings'))\n",
    "\n",
    "    # even though method is designed for Euclidean distances, we can use it for cosine distances, as done here\n",
    "    # to do so, we normalize input vectors first, then Euclidean distance is nothing else, but sqrt(2)*sqrt(cosine_distance), and sqrt is a monotone transformation\n",
    "    brpLSH = BucketedRandomProjectionLSH(inputCol=\"normed_embeddings\", outputCol=\"hashes\", seed=42, bucketLength=12.0, numHashTables=20)\n",
    "    brpLSHmodel = brpLSH.fit(df)\n",
    "\n",
    "    return brpLSHmodel, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
