{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scientific concepts project report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In scientific community it is important to updated about recent works and concepts, that are the **State-of-the-art** now. As more and more papers getting published every day, it is hard to follow all of them even in particular area of interest. With this project we want to provide the academic researcher with personilized list of scientific concepts studying of which would enhance their research capabilities. \n",
    "\n",
    "**Model user** is an academic researcher in the field of biomedicine. Said person has\n",
    "some prior (domain) knowledge that I consider as a set of concepts that person selfclaim to be comfortable with. \n",
    "\n",
    "**Major assumption** that I make is that \"scientific concepts studying of which would\n",
    "enhance their research capabilities\" are the ones that respective community of\n",
    "researches deems important, while notion of importance can be deduced in the form of\n",
    "numeric score, based on what they (researchers) write in scientific publications. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data for this project is taken from **Semantic Scholar Open Research Corpus**. Semantic Scholar is an open scientific paper search engine that connects relevant papers and extract useful information like abstracts, figures and entities of the paper. Since 2018 it includes more than 40 million papers on Biomedicine, Neuroscience and Computer Science. The data is avalible for downloading by [public API](https://api.semanticscholar.org/corpus/). \n",
    "\n",
    "The data is provided in JSON format with such fields: \n",
    "* id\n",
    "* title\n",
    "* paperAbstruct\n",
    "* entities\n",
    "* s2Url\n",
    "* s2PdfUrl\n",
    "* pdfUrls\n",
    "* authors\n",
    "* inCitations\n",
    "* outCitations\n",
    "* year\n",
    "* venue\n",
    "* journalName\n",
    "* journalVolume\n",
    "* journalPages\n",
    "* sources\n",
    "* doi\n",
    "* doiUrl\n",
    "* pmid\n",
    "\n",
    "For this project we only use title, abstruct, year, entities and id fields. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary steps recap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to execute code presented in this report, all (!) preliminary steps listed in readme.md required to be completed first. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installed and configured"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Java 8\n",
    "* Hadoop 3.1.2\n",
    "* Spark 2.4.3 (Pre-built with user-provided Apache Hadoop)\n",
    "* Python 3.6.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As well as Python libraries listed under **requirements.txt**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloaded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Semantic Scholar Open Research Corpus\n",
    "* FastText pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HDFS and Yarn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Name-nodes formatted\n",
    "* Daemons started (start-dfs.sh, start-yarn.sh)\n",
    "* Data converted to parquet and pushed to HDFS under scholar_data/base.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: you will need daemons up and running while executing code in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data\n",
    "\n",
    "#### If you have done the preliminary steps you can skip this part (you should have parque data saved) .\n",
    "\n",
    "Download by running from project folder (creates data/ subfolder and stores there):\n",
    "```\n",
    "sh bash/get_data.sh\n",
    "```\n",
    "\n",
    "Or by following instructions on the website. In later case, make sure to remove sample file sample-S2-records.gz in order to avoid data duplication."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can convert data from set of compressed jsons to parquet and store to HDFS. This is by far the most time consuming step, may take several hours to complete. On the bright side, due to to conversion, all consequent operations become rather fast.\n",
    "```\n",
    "python src/0_convert_to_parquet_store_to_hdfs.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **0_convert_to_parquet_store_to_hdfs.py** file selects 'id', 'title', 'year', 'entities', 'paperAbstract' fields and converts data to spark.sql parquet format optimized to work with big table data on pyspark. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_data = True  # change to False, if you want to rerun all cells for demo, but have made preprocessing already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirpath = os.getcwd()\n",
    "model_bin_path = f'{dirpath}/fasttext/crawl-300d-2M-subword.bin'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/usr/local/spark/spark-2.4.3-bin-without-hadoop'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "import pyspark\n",
    "findspark.find() # solely to check whether path is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SQLContext\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.ml.feature import BucketedRandomProjectionLSH, BucketedRandomProjectionLSHModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fasttext\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pivot_keyterms(df):\n",
    "    # year - single integer, entities - list of strings (keyterms)\n",
    "    # keep only papers for periods (years) that fully passed\n",
    "    # make separate row for each keyterm\n",
    "    df = df.select('year', 'entities').\\\n",
    "        filter(\"year < 2019\").\\\n",
    "        withColumn('entities', F.explode('entities'))\n",
    "    \n",
    "    # turn all keyterms to lowercase\n",
    "    df = df.withColumn('entities', F.lower(F.col('entities')))\n",
    "    \n",
    "    # cut number formatting: 10,000 -> 10000\n",
    "    df = df.withColumn('entities', F.regexp_replace(F.col('entities'), '(\\d)[, ](\\d{3})', '$1$2'))\n",
    "    \n",
    "    # treat comma separated values as a different keyterms (even though they were posed as a unit)\n",
    "    df = df.withColumn('entities', F.explode(F.split('entities', '[,]')))\n",
    "    \n",
    "    # substitute underscore with a space as a means of separation\n",
    "    # get rid of excessive spaces\n",
    "    df = df.withColumn('entities', F.regexp_replace(F.col('entities'), '_', ' '))\n",
    "    df = df.withColumn('entities', F.trim(F.col('entities')))\n",
    "    df = df.withColumn('entities', F.regexp_replace(F.col('entities'), '\\s+', ' '))\n",
    "\n",
    "    # keep only keyterms that contain alpha-numeric values and spaces\n",
    "    # retrieve counts for each keyterm for each year\n",
    "    # \\w also includes uderscore, but we handled it earlier\n",
    "    df = df.filter(~(F.col('entities').rlike('[^\\w\\s]'))).\\\n",
    "        groupby('entities').\\\n",
    "        pivot('year').\\\n",
    "        count().\\\n",
    "        sort('entities')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens_frequency(df, token_min_length=3, token_min_count=11, enhance_factor=100000.0):\n",
    "    # discard suspiciously short tokens \n",
    "    df = df.filter(f'LENGTH(entities) >= {token_min_length}')\n",
    "\n",
    "    # gather column names linked to years\n",
    "    col_years = [col_name for col_name in df.columns]\n",
    "    col_years.remove('entities')\n",
    "\n",
    "    # Find peak usage of token across the years\n",
    "    # https://stackoverflow.com/questions/40874657/pyspark-compute-row-maximum-of-the-subset-of-columns-and-add-to-an-exisiting-da\n",
    "    minf = F.lit(float(\"-inf\"))\n",
    "    df = df.withColumn(\"year_max\", F.greatest(*[F.coalesce(F.col(year), minf) for year in col_years]))\n",
    "\n",
    "    # forget about tokens that have never been really used\n",
    "    df = df.filter(f'year_max >= {token_min_count}').drop('year_max')\n",
    "\n",
    "    # find total number of \"valid\" tokens used on each year\n",
    "    df = df.join(df.groupby().sum(*col_years))\n",
    "\n",
    "    # retrieve token frequency (times common coefficient) for each year\n",
    "    # coefficient is to make sure we do not limitations of float precision too hard\n",
    "    for year in col_years:\n",
    "        df = df.withColumn(year, enhance_factor*F.col(year) / F.col(f'sum({year})')).drop(f'sum({year})')\n",
    "    \n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(pd_df, emb_length=300):\n",
    "    # retrieve embeding for each keyterm\n",
    "    model = fasttext.load_model(model_bin_path)\n",
    "    pd_df['embeddings'] = pd_df['entities'].apply(model.__getitem__)\n",
    "    del model\n",
    "\n",
    "    # append embeddings as another column to the data frame \n",
    "    emb_components = pd.DataFrame(pd_df['embeddings'].tolist(), columns=[f'v{i}' for i in range(emb_length)])\n",
    "    pd_df['embeddings'] = pd_df['embeddings'].apply(lambda x: list(x))\n",
    "    pd_df = pd.concat([pd_df[:], emb_components[:]], axis=1) \n",
    "    return pd_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_lsh_model(df, seed=42, bucketLength=12.0, numHashTables=20):\n",
    "    def make_normed_vector(x):\n",
    "        # make L2-norm of the vector to be unit\n",
    "        x_np = np.array(x, dtype=np.float64)\n",
    "        x_np = x_np / np.linalg.norm(x_np)\n",
    "        return Vectors.dense(x_np)\n",
    "    \n",
    "    df = df.select('entities', 'embeddings')\n",
    "\n",
    "    to_vector = F.udf(lambda x: Vectors.dense(x), VectorUDT())\n",
    "    to_normed_vector = F.udf(make_normed_vector, VectorUDT())\n",
    "\n",
    "    df = df.withColumn('normed_embeddings', to_normed_vector('embeddings'))\n",
    "    df = df.withColumn('embeddings', to_vector('embeddings'))\n",
    "\n",
    "    # even though method is designed for Euclidean distances, we can use it for cosine distances, as done here\n",
    "    # to do so, we normalize input vectors first, then Euclidean distance is nothing else, but sqrt(2)*sqrt(cosine_distance), and sqrt is a monotone transformation\n",
    "    brpLSH = BucketedRandomProjectionLSH(inputCol=\"normed_embeddings\", outputCol=\"hashes\", seed=seed, bucketLength=bucketLength, numHashTables=numHashTables)\n",
    "    brpLSHmodel = brpLSH.fit(df)\n",
    "\n",
    "    return brpLSHmodel, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_np_array(x):\n",
    "    return Vectors.dense(x/np.linalg.norm(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if preprocess_data:\n",
    "    spark = SparkSession.builder.\\\n",
    "        master('yarn').\\\n",
    "        appName('scholar').\\\n",
    "        getOrCreate()\n",
    "\n",
    "    sc = spark.sparkContext\n",
    "    sqlContext = SQLContext(sc)\n",
    "\n",
    "    # enables cartesian (!) join\n",
    "    spark.conf.set(\"spark.sql.crossJoin.enabled\", \"true\")\n",
    "    spark.conf.set(\"spark.sql.broadcastTimeout\", \"1800\")   # ensure that stage 11 does not terminate on timeout \n",
    "\n",
    "    df = sqlContext.read.format('parquet').load('hdfs:/scholar_data/base.parquet')\n",
    "    df_pivoted = pivot_keyterms(df)\n",
    "    df_pivoted.write.save('hdfs:/scholar_data/tokens_count_by_year.parquet', format='parquet', mode='overwrite')\n",
    "\n",
    "    df_token_freq = get_tokens_frequency(df_pivoted)\n",
    "    df_token_freq.write.save('hdfs:/scholar_data/tokens_freq_by_year.parquet', format='parquet', mode='overwrite')\n",
    "\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if preprocess_data:\n",
    "    spark = SparkSession.builder.\\\n",
    "        master('local[2]').\\\n",
    "        appName('scholar').\\\n",
    "        getOrCreate()\n",
    "\n",
    "    sc = spark.sparkContext\n",
    "    sqlContext = SQLContext(sc)\n",
    "\n",
    "    df_token_freq = sqlContext.read.format('parquet').load('hdfs:/scholar_data/tokens_freq_by_year.parquet')\n",
    "    pd_df_entities = df_token_freq.select('entities').toPandas()\n",
    "\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if preprocess_data:\n",
    "    pd_df_embeddings = get_embeddings(pd_df_entities)\n",
    "    pa_df_embeddings = pa.Table.from_pandas(pd_df_embeddings)\n",
    "\n",
    "    fs = pa.hdfs.connect()\n",
    "    with fs.open('hdfs:/scholar_data/token_embeddings.parquet', 'wb') as target:\n",
    "        pq.write_table(pa_df_embeddings, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if preprocess_data:\n",
    "    spark = SparkSession.builder.\\\n",
    "        master('local[2]').\\\n",
    "        appName('scholar').\\\n",
    "        getOrCreate()\n",
    "\n",
    "    sc = spark.sparkContext\n",
    "    sqlContext = SQLContext(sc)\n",
    "\n",
    "    df_embeddings = sqlContext.read.format('parquet').load('hdfs:/scholar_data/token_embeddings.parquet')\n",
    "    brpLSHmodel, df_normed_embeddings = fit_lsh_model(df_embeddings)\n",
    "    brpLSHmodel.write().overwrite().save('hdfs:/scholar_model/brpLSH_model')\n",
    "    df_normed_embeddings.write.save('hdfs:/scholar_data/token_normed_vector_embeddings.parquet', format='parquet', mode='overwrite')\n",
    "\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: move forecast step from tsa notebook here and add comparison "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once loaded, you can enter your search queriers (word or several words). To exit gracefully type 'stop'.\n",
    "\n",
    "If you want to run it after reloading notebook with data already preprocessed, set **preprocess_data=False** back at top, that way you can skip unnecessary repetiton of data preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed! You can start!\n",
      "i have a bream\n",
      "+----------------------+-------+-----+\n",
      "|entities              |distCol|score|\n",
      "+----------------------+-------+-----+\n",
      "|hay fever             |0.757  |1.346|\n",
      "|sea bream             |0.59   |1.127|\n",
      "|tobacco mosaic virus  |0.78   |0.652|\n",
      "|pancreatic juice      |0.78   |0.474|\n",
      "|lingual thyroid       |0.77   |0.332|\n",
      "|grape juice           |0.772  |0.255|\n",
      "|pituitary thyroid axis|0.781  |0.148|\n",
      "|vaginal atrophy       |0.764  |0.113|\n",
      "|orange juice          |0.771  |0.113|\n",
      "|grapefruit juice      |0.774  |0.101|\n",
      "|cranberry juice       |0.773  |0.095|\n",
      "|raspberry juice       |0.749  |0.071|\n",
      "|pineapple juice       |0.777  |0.065|\n",
      "|carrot juice          |0.757  |0.059|\n",
      "|stoma cap             |0.776  |0.047|\n",
      "|iris atrophy          |0.778  |0.047|\n",
      "|centriacinar emphysema|0.777  |0.036|\n",
      "|mango juice           |0.738  |0.036|\n",
      "|moloney leukemia virus|0.777  |0.024|\n",
      "|dnajb1 protein        |0.781  |0.024|\n",
      "+----------------------+-------+-----+\n",
      "\n",
      "stop\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.\\\n",
    "    master('yarn').\\\n",
    "    appName('scholar').\\\n",
    "    getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "print(\"Loading...\", end='')\n",
    "df_normed_embeddings = sqlContext.read.format('parquet').load('hdfs:/scholar_data/token_normed_vector_embeddings.parquet').select('entities', 'normed_embeddings')\n",
    "df_token_freq = sqlContext.read.format('parquet').load('hdfs:/scholar_data/tokens_freq_by_year.parquet').select('entities', F.col('2018').alias('score'))\n",
    "ftmodel = fasttext.load_model(model_bin_path)\n",
    "brpLSHmodel = BucketedRandomProjectionLSHModel.load('hdfs:/scholar_model/brpLSH_model')\n",
    "print(\"Completed! You can start!\")\n",
    "\n",
    "try:\n",
    "    neighbors_num = 20\n",
    "    dist_cutoff = 1\n",
    "    token = ''\n",
    "    while token != 'stop':\n",
    "        token = input()\n",
    "        token = token.lower()\n",
    "        if token == 'stop':\n",
    "            break\n",
    "        token_vector = norm_np_array(ftmodel[token])\n",
    "\n",
    "        search_result = brpLSHmodel.approxNearestNeighbors(df_normed_embeddings, token_vector, neighbors_num).select('entities', 'distCol').filter(f'distCol < {dist_cutoff}')\n",
    "        search_result = search_result.join(df_token_freq, 'entities', how='left')\n",
    "        search_result = search_result.select('entities', *[F.round(F.col(c), 3).alias(c) for c in ['distCol', 'score']])\n",
    "        search_result.orderBy('score', ascending=False).show(n=neighbors_num, truncate=False)\n",
    "except:\n",
    "    print(\"Error occured while executing demo loop, stop spark session gracefully\")\n",
    "    \n",
    "del df_normed_embeddings    \n",
    "del df_token_freq    \n",
    "del ftmodel\n",
    "del brpLSHmodel\n",
    "    \n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
